            cudaStreamWaitEvent(launchStream, m_captureReadyEvent->get(), 0);
        }

        // Validate freshness after wait; if frame is stale vs last input, fetch a fresh one.
        if (m_capture) {
            uint64_t minQpc = m_pendingInputQpc.load(std::memory_order_acquire);
            if (minQpc != 0) {
                uint64_t lastQpc = m_capture->GetLastPresentQpc();
                if (lastQpc != 0 && lastQpc < minQpc) {
                    // Need a newer frame; do a synchronous capture and re-wait on event.
                    (void)scheduleNextFrameCapture(true);
                    if (m_captureReadyEvent && m_captureReadyEvent->get()) {
                        cudaStreamWaitEvent(launchStream, m_captureReadyEvent->get(), 0);
                    }
                } else if (lastQpc != 0) {
                    m_pendingInputQpc.store(0, std::memory_order_release);
                }
            }
        }

        bool shouldDispatchMovement = m_config.enableDetection && !ctx.detection_paused.load();
        m_allowMovement.store(shouldDispatchMovement, std::memory_order_release);

        // Launch the graph using the stabilized m_captureBuffer pointer
        cudaError_t launchErr = cudaGraphLaunch(m_graphExec, launchStream);
        if (launchErr != cudaSuccess) {
            std::cerr << "[UnifiedGraph] Graph launch failed: "
                      << cudaGetErrorString(launchErr) << std::endl;
            m_allowMovement.store(false, std::memory_order_release);
            m_frameInFlight.store(false, std::memory_order_release);
            {
                std::lock_guard<std::mutex> g(m_inflightMutex);
            }
            m_inflightCv.notify_all();
            return false;
        }

        // Mark frame as consumed; the next capture will update the ring
        m_captureState.store(CaptureState::CONSUMED, std::memory_order_release);
    } else {
        if (!executeNormalPipeline(launchStream)) {
            m_allowMovement.store(false, std::memory_order_release);
            m_frameInFlight.store(false, std::memory_order_release);
            {
                std::lock_guard<std::mutex> g(m_inflightMutex);
            }
            m_inflightCv.notify_all();
            if (outReason) *outReason = FrameFailureReason::NO_FRAME_READY;
            return false;
        }
    }

    // Update performance metrics
    auto frameEnd = std::chrono::steady_clock::now();
    auto latencyMs = std::chrono::duration<double, std::milli>(frameEnd - frameStart).count();
    
    m_perfMetrics.totalFrames++;
    m_perfMetrics.totalLatencyMs += latencyMs;
    m_perfMetrics.logIfNeeded("[Perf]");
    
    if (outReason) *outReason = FrameFailureReason::NONE;

    // Periodic CUDA memory maintenance for long-running sessions
    static auto lastTrim = std::chrono::steady_clock::now();
    auto now = std::chrono::steady_clock::now();
    if (std::chrono::duration_cast<std::chrono::minutes>(now - lastTrim).count() >= 10) {
        cudaMemPoolTrimTo(nullptr, 0);
        // Trim graph memory pool for the current device (0 used elsewhere in code)
        cudaDeviceGraphMemTrim(0);
        lastTrim = now;
    }

    return true;
}

bool UnifiedGraphPipeline::executeNormalPipeline(cudaStream_t stream) {
    auto& ctx = AppContext::getInstance();

    cudaStream_t activeStream = stream ? stream : (m_pipelineStream ? m_pipelineStream->get() : nullptr);
    if (!activeStream) {
        return false;
    }

    FrameFailureReason ensureReason = ensureFrameReady();
    if (ensureReason != FrameFailureReason::NONE) {
        return false; // Frame not ready
    }

    bool shouldRunDetection = m_config.enableDetection && !ctx.detection_paused.load();

    if (shouldRunDetection) {
        // Ensure any pending capture copy is visible to the pipeline stream
        if (m_captureReadyEvent && m_captureReadyEvent->get()) {
            cudaStreamWaitEvent(activeStream, m_captureReadyEvent->get(), 0);
        }
        if (!performPreprocessing()) {
            return false;
        }

        (void)scheduleNextFrameCapture(false); // Non-blocking schedule

        if (!performInference()) {
            return false;
        }

        performIntegratedPostProcessing(activeStream);
        performTargetSelection(activeStream);

        if (!m_mouseMovementUsesMappedMemory) {
            cudaMemcpyAsync(m_h_movement->get(), m_smallBufferArena.mouseMovement,
                           sizeof(MouseMovement), cudaMemcpyDeviceToHost, activeStream);
        }
    }

    m_allowMovement.store(shouldRunDetection, std::memory_order_release);
    if (shouldRunDetection) {
        if (!enqueueFrameCompletionCallback(activeStream)) {
            m_allowMovement.store(false, std::memory_order_release);
            return false;
        }
    } else {
        (void)scheduleNextFrameCapture(false); // Non-blocking schedule

        cudaError_t streamState = cudaStreamQuery(activeStream);
        if (streamState == cudaSuccess) {
            clearMovementData();
            m_frameInFlight.store(false, std::memory_order_release);
            {
                std::lock_guard<std::mutex> g(m_inflightMutex);
            }
            m_inflightCv.notify_all();
