# ğŸ¯ needaimbot - Ultra-High Performance AI Targeting System

<div align="center">

[![CUDA](https://img.shields.io/badge/CUDA-12.8-green.svg)](https://developer.nvidia.com/cuda-toolkit)
[![TensorRT](https://img.shields.io/badge/TensorRT-10.8-blue.svg)](https://developer.nvidia.com/tensorrt)
[![C++](https://img.shields.io/badge/C++-17-orange.svg)](https://en.cppreference.com/w/cpp/17)
[![Performance](https://img.shields.io/badge/Latency-<3ms-red.svg)](https://github.com)
[![License](https://img.shields.io/badge/License-MIT-purple.svg)](LICENSE)

**Real-time AI-Powered Targeting System with Sub-3ms Response Time**

</div>

---

## ğŸ“Š Executive Summary

**needaimbot**ì€ ê·¹í•œì˜ ì„±ëŠ¥ì„ ì¶”êµ¬í•˜ëŠ” ì°¨ì„¸ëŒ€ AI íƒ€ê²ŸíŒ… ì‹œìŠ¤í…œìœ¼ë¡œ, RTX 4060 í™˜ê²½ì—ì„œ **í•œ ì‚¬ì´í´ë‹¹ 3ms ì´ë‚´**ì˜ ì‘ë‹µ ì‹œê°„ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ì™„ë²½í•œ ë¹„ë™ê¸° ì²˜ë¦¬ì™€ Zero-Copy ë©”ëª¨ë¦¬ ì•„í‚¤í…ì²˜ë¥¼ í†µí•´ **ìº¡ì²˜ 0ms, ì¶”ë¡  3ms**ì˜ í˜ì‹ ì ì¸ ì„±ëŠ¥ì„ êµ¬í˜„í•˜ë©°, ì—ì„ë´‡ì´ í™œì„±í™”ë  ë•Œë§Œ GPU ìì›ì„ ì‚¬ìš©í•˜ëŠ” **ê·¹í•œì˜ íš¨ìœ¨ì„±**ì„ ìë‘í•©ë‹ˆë‹¤.

### ğŸš€ í•µì‹¬ ì„±ëŠ¥ ì§€í‘œ (RTX 4060 ê¸°ì¤€)

| ì¸¡ì • í•­ëª© | ì„±ëŠ¥ ìˆ˜ì¹˜ | ê¸°ìˆ ì  íŠ¹ì§• |
|-----------|-----------|------------|
| **í™”ë©´ ìº¡ì²˜** | **0ms** | Zero-Copy D3D11-CUDA Interop |
| **AI ì¶”ë¡ ** | **<3ms** | TensorRT 10.8 FP16 ìµœì í™” |
| **ì „ì²´ ë ˆì´í„´ì‹œ** | **<3ms** | ì™„ë²½í•œ ë¹„ë™ê¸° íŒŒì´í”„ë¼ì¸ |
| **GPU ì‚¬ìš©ë¥ ** | **5-15%** (í™œì„± ì‹œ) | ì˜¨ë””ë§¨ë“œ ìì› í™œìš© |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©** | **~1.2GB VRAM** | íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ í’€ë§ |
| **ì²˜ë¦¬ í”„ë ˆì„** | **300+ FPS** | Triple Buffer ì‹œìŠ¤í…œ |

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### 1. **Unified GPU Pipeline Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   UNIFIED GRAPH PIPELINE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  [D3D11 Capture] â”€â”€Zero-Copyâ”€â”€> [CUDA Buffer]                â”‚
â”‚         â†“                            â†“                        â”‚
â”‚  [CUDA Preprocessing] â”€â”€â”€â”€â”€â”€> [TensorRT Engine]              â”‚
â”‚         â†“                            â†“                        â”‚
â”‚  [Post-Processing] â”€â”€â”€â”€â”€â”€â”€â”€> [Target Selection]              â”‚
â”‚         â†“                            â†“                        â”‚
â”‚  [Triple Buffer] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> [Mouse Control]                 â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. **í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ**

#### **A. Zero-Latency Capture System**
- **D3D11-CUDA Interoperability**: CPU ê°œì… ì—†ëŠ” ì§ì ‘ GPU ë©”ëª¨ë¦¬ ë§¤í•‘
- **Desktop Duplication API**: í•˜ë“œì›¨ì–´ ê°€ì† í™”ë©´ ìº¡ì²˜
- **CUDA Graphics Resource**: í…ìŠ¤ì²˜ ì§ì ‘ ì•¡ì„¸ìŠ¤ë¡œ ë©”ëª¨ë¦¬ ë³µì‚¬ ì œê±°
- **ì‹¤ì‹œê°„ ì„±ëŠ¥**: ìº¡ì²˜ ì˜¤ë²„í—¤ë“œ ì™„ì „ ì œê±° (0ms)

#### **B. AI Inference Pipeline**
- **TensorRT 10.8 ìµœì í™”**:
  - Dynamic Shape ì§€ì›
  - FP16/INT8 ìë™ ìº˜ë¦¬ë¸Œë ˆì´ì…˜
  - ì»¤ë„ ìë™ ìœµí•© (Kernel Fusion)
  - ë©€í‹°ìŠ¤íŠ¸ë¦¼ ì‹¤í–‰
- **YOLO ì•„í‚¤í…ì²˜ ì§€ì›**: YOLOv8/v9/v10/v11/v12
- **ìµœì í™”ëœ í›„ì²˜ë¦¬**:
  - GPU ê¸°ë°˜ NMS (Non-Maximum Suppression)
  - CUDA ì»¤ë„ ë””ì½”ë”©
  - ë³‘ë ¬ íƒ€ê²Ÿ í•„í„°ë§

#### **C. Triple Buffer System**
```cpp
struct TripleBuffer {
    std::atomic<int> captureIdx{0};   // ìº¡ì²˜ ë²„í¼ ì¸ë±ìŠ¤
    std::atomic<int> processIdx{0};   // ì²˜ë¦¬ ë²„í¼ ì¸ë±ìŠ¤
    std::atomic<int> displayIdx{0};   // í‘œì‹œ ë²„í¼ ì¸ë±ìŠ¤
    
    SimpleCudaMat buffers[3];         // íŠ¸ë¦¬í”Œ ë²„í¼
    cudaEvent_t ready_events[3];      // ë™ê¸°í™” ì´ë²¤íŠ¸
};
```
- **ì™„ë²½í•œ ë¹„ë™ê¸° ì²˜ë¦¬**: ìº¡ì²˜/ì²˜ë¦¬/í‘œì‹œ ë™ì‹œ ì‹¤í–‰
- **Zero-Wait íŒŒì´í”„ë¼ì¸**: ë²„í¼ ì „í™˜ ì‹œ ëŒ€ê¸° ì‹œê°„ ì œê±°
- **Pinned Memory**: Host-Device ì „ì†¡ ìµœì í™”

### 3. **ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”**

#### **CUDA Memory Pool Architecture**
```cpp
// RAII ê¸°ë°˜ ìë™ ë©”ëª¨ë¦¬ ê´€ë¦¬
template<typename T>
class CudaMemory {
    std::unique_ptr<T, CudaDeleter> ptr;
    size_t size;
public:
    explicit CudaMemory(size_t n);
    T* get() const { return ptr.get(); }
    void reset();
};
```

- **ë©”ëª¨ë¦¬ í’€ë§**: ë™ì  í• ë‹¹ ì˜¤ë²„í—¤ë“œ ì œê±°
- **Pinned Memory**: CPU-GPU ì „ì†¡ ìµœì í™”
- **Unified Memory**: ìë™ í˜ì´ì§€ ë§ˆì´ê·¸ë ˆì´ì…˜
- **RAII íŒ¨í„´**: ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬

### 4. **ì…ë ¥ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜**

#### **ë‹¤ì¤‘ í•˜ë“œì›¨ì–´ ì§€ì›**
```cpp
class InputMethod {
    virtual void move(int x, int y) = 0;
    virtual void press() = 0;
    virtual void release() = 0;
};

// ì§€ì› ë“œë¼ì´ë²„
- GhubInputMethod     // Logitech G-Hub
- RazerInputMethod    // Razer Synapse
- SerialInputMethod   // Arduino/Custom HW
- KmboxNetMethod      // Network Hardware
- MakcuInputMethod    // Professional HW
```

---

## ğŸ”¬ ê¸°ìˆ ì  ìƒì„¸ ë¶„ì„

### 1. **CUDA ì»¤ë„ ìµœì í™”**

#### **Unified Preprocessing Kernel**
```cuda
__global__ void unifiedPreprocessKernel(
    uint8_t* input,    // BGRA ì…ë ¥
    float* output,     // CHW ì¶œë ¥
    int width, int height,
    float scale, float* mean, float* std
) {
    // Coalesced Memory Access
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    // Warp-level ìµœì í™”
    if (idx < width * height) {
        // BGRA to RGB + Normalize + Layout Transform
        // ë‹¨ì¼ íŒ¨ìŠ¤ë¡œ ëª¨ë“  ì „ì²˜ë¦¬ ìˆ˜í–‰
    }
}
```

**ìµœì í™” ê¸°ë²•**:
- **Coalesced Memory Access**: 32-byte ì •ë ¬ëœ ë©”ëª¨ë¦¬ ì ‘ê·¼
- **Warp Divergence ìµœì†Œí™”**: ì¡°ê±´ë¬¸ ì œê±°
- **Shared Memory í™œìš©**: íƒ€ì¼ ê¸°ë°˜ ì²˜ë¦¬
- **Grid-Stride Loop**: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬

### 2. **TensorRT ì—”ì§„ ìµœì í™”**

#### **Dynamic Shape Optimization**
```cpp
// ë™ì  ì…ë ¥ í¬ê¸° ì§€ì›
auto profile = builder->createOptimizationProfile();
profile->setDimensions("input", 
    OptProfileSelector::kMIN, Dims4{1, 3, 256, 256});
profile->setDimensions("input", 
    OptProfileSelector::kOPT, Dims4{1, 3, 320, 320});
profile->setDimensions("input", 
    OptProfileSelector::kMAX, Dims4{1, 3, 640, 640});
```

#### **Precision Calibration**
- **FP16 ìë™ ë³€í™˜**: 2ë°° ì²˜ë¦¬ëŸ‰, 50% ë©”ëª¨ë¦¬ ì ˆê°
- **INT8 ìº˜ë¦¬ë¸Œë ˆì´ì…˜**: 4ë°° ì²˜ë¦¬ëŸ‰ (ì§€ì› GPU)
- **Layer Fusion**: Conv+BN+ReLU ìë™ ìœµí•©

### 3. **ë¹„ë™ê¸° ì‹¤í–‰ íŒŒì´í”„ë¼ì¸**

```cpp
// 3-Stage Async Pipeline
cudaStream_t streams[3];

// Stage 1: Capture (Stream 0)
cudaGraphicsMapResources(1, &resource, streams[0]);

// Stage 2: Process (Stream 1)
preprocessKernel<<<grid, block, 0, streams[1]>>>();
context->enqueueV3(streams[1]);

// Stage 3: Output (Stream 2)
cudaMemcpyAsync(host, device, size, streams[2]);

// ì´ë²¤íŠ¸ ê¸°ë°˜ ë™ê¸°í™”
cudaEventRecord(events[i], streams[i]);
```

### 4. **ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ì‹œìŠ¤í…œ**

```cpp
class PerformanceMetrics {
    std::atomic<float> captureTime{0.0f};
    std::atomic<float> inferenceTime{0.0f};
    std::atomic<float> postprocessTime{0.0f};
    std::atomic<float> totalLatency{0.0f};
    
    void measureKernel(cudaEvent_t start, cudaEvent_t end);
};
```

---

## ğŸ’¡ í˜ì‹ ì  ê¸°ëŠ¥

### 1. **Adaptive Resource Management**
- **ì˜¨ë””ë§¨ë“œ GPU í™œì„±í™”**: ì—ì„ë´‡ í™œì„± ì‹œì—ë§Œ GPU ì‚¬ìš©
- **ë™ì  í•´ìƒë„ ì¡°ì •**: ì„±ëŠ¥ì— ë”°ë¥¸ ìë™ í•´ìƒë„ ìŠ¤ì¼€ì¼ë§
- **ìŠ¤ë§ˆíŠ¸ íƒ€ê²Ÿ í•„í„°ë§**: GPU ê¸°ë°˜ ì‹¤ì‹œê°„ ìš°ì„ ìˆœìœ„ ê³„ì‚°

### 2. **Advanced Tracking System**
```cpp
// GPU Kalman Filter êµ¬í˜„
__device__ void kalmanPredict(
    KalmanState* state,
    float dt
) {
    // ìƒíƒœ ì˜ˆì¸¡ (ìœ„ì¹˜, ì†ë„, ê°€ì†ë„)
    state->x += state->vx * dt + 0.5f * state->ax * dt * dt;
    state->y += state->vy * dt + 0.5f * state->ay * dt * dt;
    
    // ê³µë¶„ì‚° ì—…ë°ì´íŠ¸
    updateCovariance(state, dt);
}
```

### 3. **Multi-Model Hot Swapping**
- ì‹¤ì‹œê°„ ëª¨ë¸ êµì²´ ì§€ì›
- ë©”ëª¨ë¦¬ ì‚¬ì „ í• ë‹¹ìœ¼ë¡œ êµì²´ ì‹œ ì§€ì—° ì œê±°
- ìë™ TensorRT ì—”ì§„ ìºì‹±

---

## ğŸ› ï¸ ë¹Œë“œ ë° ì„¤ì¹˜

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

#### **ìµœì†Œ ì‚¬ì–‘**
- **GPU**: NVIDIA RTX 2060 (Turing ì•„í‚¤í…ì²˜)
- **CUDA**: 12.8
- **RAM**: 8GB
- **OS**: Windows 10 20H2+

#### **ê¶Œì¥ ì‚¬ì–‘**
- **GPU**: NVIDIA RTX 4060 ì´ìƒ
- **CUDA**: 12.8
- **RAM**: 16GB
- **OS**: Windows 11 22H2+

### ë¹ ë¥¸ ì‹œì‘ (Pre-built Binary)

**ìµœì‹  ë¦´ë¦¬ì¦ˆ ë‹¤ìš´ë¡œë“œ**: [Mega.nzì—ì„œ ë‹¤ìš´ë¡œë“œ](https://mega.nz/file/MWkk0LSD#PbnofZnIjHYDKNH96oy4oN1_yPEBx7vR7w0qt07cu04)

```bash
# 1. ìœ„ ë§í¬ì—ì„œ ë‹¤ìš´ë¡œë“œ
# 2. ì••ì¶• í•´ì œ
# 3. needaimbot.exe ì‹¤í–‰ (ê´€ë¦¬ì ê¶Œí•œ ê¶Œì¥)
```

### ì†ŒìŠ¤ ì½”ë“œ ë¹Œë“œ

```bash
# 1. ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/needitem/needaimbot.git
cd needaimbot

# 2. ì˜ì¡´ì„± ì„¤ì¹˜
# - CUDA Toolkit 12.8
# - Visual Studio 2022
# - Windows SDK 10.0.26100.0+

# 3. ë¹Œë“œ
msbuild needaimbot.sln /p:Configuration=Release /p:Platform=x64

# 4. TensorRT ì—”ì§„ ìƒì„± (ì²« ì‹¤í–‰ ì‹œ ìë™)
./x64/Release/needaimbot.exe
```

---

## ğŸ“ˆ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### GPUë³„ ì„±ëŠ¥ ë¹„êµ

| GPU Model | Capture | Inference | Total | FPS |
|-----------|---------|-----------|-------|-----|
| RTX 4090 | 0ms | 1.8ms | <2ms | 500+ |
| RTX 4080 | 0ms | 2.1ms | <2.5ms | 400+ |
| RTX 4070 Ti | 0ms | 2.5ms | <3ms | 350+ |
| **RTX 4060** | **0ms** | **2.8ms** | **<3ms** | **300+** |
| RTX 3080 | 0ms | 3.5ms | <4ms | 250+ |
| RTX 3070 | 0ms | 4.2ms | <5ms | 200+ |

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„

```
VRAM ì‚¬ìš© ë‚´ì—­ (RTX 4060):
â”œâ”€â”€ TensorRT Engine: 450MB
â”œâ”€â”€ CUDA Buffers: 320MB
â”œâ”€â”€ Triple Buffer: 180MB
â”œâ”€â”€ Preprocessing: 120MB
â”œâ”€â”€ Post-processing: 80MB
â””â”€â”€ Misc: 50MB
ì´ê³„: ~1.2GB
```

---

## ğŸ”§ ê³ ê¸‰ ì„¤ì •

### config.ini ìµœì í™” ì˜ˆì‹œ

```ini
[Performance]
detection_resolution = 320      # ë‚®ì„ìˆ˜ë¡ ë¹ ë¦„
max_detections = 30             # íƒ€ê²Ÿ ìˆ˜ ì œí•œ
confidence_threshold = 0.35     # ë†’ì„ìˆ˜ë¡ ì •í™•
enable_fp16 = true              # FP16 ê°€ì†
triple_buffer = true            # ë¹„ë™ê¸° ì²˜ë¦¬

[GPU]
cuda_device = 0                 # GPU ì„ íƒ
stream_priority = high          # ìŠ¤íŠ¸ë¦¼ ìš°ì„ ìˆœìœ„
memory_pool_size = 2048         # MB ë‹¨ìœ„

[Optimization]
kernel_fusion = true            # ì»¤ë„ ìœµí•©
graph_optimization = true       # ê·¸ë˜í”„ ìµœì í™”
dynamic_shapes = true           # ë™ì  í¬ê¸° ì§€ì›
```

---

## ğŸ¯ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

### 1. **ê²½ìŸ ê²Œì´ë°**
- ì´ˆì €ì§€ì—° íƒ€ê²Ÿ ê°ì§€
- ì •ë°€í•œ ì—ì„ ë³´ì •
- ë‹¤ì¤‘ íƒ€ê²Ÿ ìš°ì„ ìˆœìœ„ ì§€ì •

### 2. **í›ˆë ¨ ë° ë¶„ì„**
- ì—ì„ íŒ¨í„´ ë¶„ì„
- ë°˜ì‘ ì‹œê°„ ì¸¡ì •
- ì •í™•ë„ í†µê³„

### 3. **ê°œë°œ ë° ì—°êµ¬**
- AI ëª¨ë¸ í…ŒìŠ¤íŠ¸
- ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹
- ì•Œê³ ë¦¬ì¦˜ ê²€ì¦

---

## ğŸ“š ê¸°ìˆ  ë¬¸ì„œ

### API Reference
```cpp
// Pipeline ì´ˆê¸°í™”
UnifiedGraphPipeline pipeline(config);
pipeline.initialize();

// ë¹„ë™ê¸° ì‹¤í–‰
pipeline.executeGraphNonBlocking(stream);

// ê²°ê³¼ íšë“
Target* targets = pipeline.getTargets();
int count = pipeline.getTargetCount();
```

### CUDA ì»¤ë„ ì¸í„°í˜ì´ìŠ¤
```cuda
// ì „ì²˜ë¦¬ ì»¤ë„
cuda_unified_preprocessing(
    uint8_t* input, float* output,
    int width, int height,
    cudaStream_t stream
);

// í›„ì²˜ë¦¬ ì»¤ë„
performNMS_gpu(
    Target* targets, int* count,
    float threshold,
    cudaStream_t stream
);
```

---

## ğŸ¤ ê¸°ì—¬ ê°€ì´ë“œ (Contributing Guide)

### ğŸ¯ ê¸°ì—¬ ë°©ë²•

#### 1. **í™˜ê²½ ì„¤ì •**

```bash
# Fork ë° Clone
git clone https://github.com/YOUR_USERNAME/needaimbot.git
cd needaimbot
git remote add upstream https://github.com/needitem/needaimbot.git

# ë¸Œëœì¹˜ ìƒì„±
git checkout -b feature/your-feature-name
```

#### 2. **ê°œë°œ í™˜ê²½ êµ¬ì¶•**

**í•„ìˆ˜ ë„êµ¬ ì„¤ì¹˜:**
```powershell
# 1. CUDA Toolkit 12.8
# https://developer.nvidia.com/cuda-12-8-0-download-archive

# 2. Visual Studio 2022
# - Desktop development with C++ ì›Œí¬ë¡œë“œ ì„ íƒ
# - Windows 10/11 SDK
# - MSVC v143

# 3. ì˜ì¡´ì„± ë‹¤ìš´ë¡œë“œ
cd needaimbot/modules
# TensorRT, cuDNN, GLFW ë“± í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```

**í”„ë¡œì íŠ¸ ì„¤ì •:**
```xml
<!-- needaimbot.vcxproj í™•ì¸ ì‚¬í•­ -->
<CudaToolkitCustomDir>$(CUDA_PATH)</CudaToolkitCustomDir>
<CudaArchitecture>sm_75;sm_80;sm_86;sm_89</CudaArchitecture>
```

#### 3. **ì½”ë“œ ì‘ì„± ê°€ì´ë“œë¼ì¸**

##### **ì„±ëŠ¥ ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸**
- [ ] CUDA í”„ë¡œíŒŒì¼ë§ ì‹¤í–‰ (`nvprof` ë˜ëŠ” Nsight)
- [ ] ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê²€ì‚¬ (`cuda-memcheck`)
- [ ] 3ms ì´ë‚´ ë ˆì´í„´ì‹œ ìœ ì§€ í™•ì¸
- [ ] GPU ì‚¬ìš©ë¥  15% ì´í•˜ í™•ì¸

##### **ì½”ë“œ ìŠ¤íƒ€ì¼**
```cpp
// íŒŒì¼ êµ¬ì¡°
// header.h
#pragma once
#include "cuda_runtime.h"

class ClassName {
public:
    explicit ClassName(Config config);
    ~ClassName();
    
    // Public methods
    void publicMethod();
    
private:
    // RAII ë©¤ë²„
    std::unique_ptr<Resource> m_resource;
    
    // CUDA ë¦¬ì†ŒìŠ¤
    CudaMemory<float> m_deviceBuffer;
};

// implementation.cpp
#include "header.h"

ClassName::ClassName(Config config) 
    : m_resource(std::make_unique<Resource>())
    , m_deviceBuffer(config.bufferSize) {
    // ì´ˆê¸°í™” ì½”ë“œ
}

// CUDA ì»¤ë„
__global__ void processKernel(
    float* input,
    float* output,
    int size
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        // Coalesced memory access
        output[idx] = process(input[idx]);
    }
}
```

#### 4. **í…ŒìŠ¤íŠ¸ ìš”êµ¬ì‚¬í•­**

```cpp
// ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ
TEST(PipelineTest, InferenceLatency) {
    UnifiedGraphPipeline pipeline(testConfig);
    
    auto start = std::chrono::high_resolution_clock::now();
    pipeline.executeGraphNonBlocking();
    cudaStreamSynchronize(0);
    auto end = std::chrono::high_resolution_clock::now();
    
    auto latency = std::chrono::duration<float, std::milli>(end - start).count();
    EXPECT_LT(latency, 3.0f); // 3ms ì´ë‚´
}

// ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
void benchmarkGPU() {
    // RTX 2060, 3060, 4060ì—ì„œ í…ŒìŠ¤íŠ¸
    for (auto gpu : supportedGPUs) {
        cudaSetDevice(gpu);
        runPerformanceTest();
    }
}
```

#### 5. **Pull Request í”„ë¡œì„¸ìŠ¤**

##### **PR ì²´í¬ë¦¬ìŠ¤íŠ¸**
```markdown
## PR ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ì½”ë“œê°€ ì»´íŒŒì¼ë˜ê³  ì‹¤í–‰ë¨
- [ ] 3ms ë ˆì´í„´ì‹œ ëª©í‘œ ë‹¬ì„±
- [ ] ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì—†ìŒ
- [ ] CUDA ì—ëŸ¬ ì²´í¬ í¬í•¨
- [ ] ê´€ë ¨ ë¬¸ì„œ ì—…ë°ì´íŠ¸
- [ ] í…ŒìŠ¤íŠ¸ ì¶”ê°€/ì—…ë°ì´íŠ¸
```

##### **ì»¤ë°‹ ë©”ì‹œì§€ ê·œì¹™**
```bash
# í˜•ì‹: <type>(<scope>): <subject>

feat(cuda): GPU ë©”ëª¨ë¦¬ í’€ êµ¬í˜„ìœ¼ë¡œ í• ë‹¹ ì˜¤ë²„í—¤ë“œ ì œê±°
perf(pipeline): ì»¤ë„ ìœµí•©ìœ¼ë¡œ ì¶”ë¡  ì‹œê°„ 15% ê°œì„ 
fix(capture): D3D11 í…ìŠ¤ì²˜ ëˆ„ìˆ˜ ë¬¸ì œ í•´ê²°
docs(readme): ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì„¹ì…˜ ì—…ë°ì´íŠ¸

# Type
- feat: ìƒˆë¡œìš´ ê¸°ëŠ¥
- fix: ë²„ê·¸ ìˆ˜ì •
- perf: ì„±ëŠ¥ ê°œì„ 
- docs: ë¬¸ì„œ ìˆ˜ì •
- style: ì½”ë“œ ìŠ¤íƒ€ì¼ ë³€ê²½
- refactor: ì½”ë“œ ë¦¬íŒ©í† ë§
- test: í…ŒìŠ¤íŠ¸ ì¶”ê°€/ìˆ˜ì •
- chore: ë¹Œë“œ í”„ë¡œì„¸ìŠ¤ ë“± ê¸°íƒ€ ë³€ê²½
```

##### **PR ì œì¶œ**
```bash
# ë³€ê²½ì‚¬í•­ ì»¤ë°‹
git add .
git commit -m "perf(cuda): optimize memory access pattern"

# upstream ë™ê¸°í™”
git fetch upstream
git rebase upstream/main

# Push
git push origin feature/your-feature-name

# GitHubì—ì„œ PR ìƒì„±
```

### ğŸ”¥ ìš°ì„ ìˆœìœ„ ê¸°ì—¬ ì˜ì—­

#### **1. ì„±ëŠ¥ ìµœì í™” (High Priority)**
- **CUDA ì»¤ë„ ìµœì í™”**
  - Warp divergence ê°ì†Œ
  - Shared memory í™œìš© ê°œì„ 
  - Memory coalescing íŒ¨í„´ ìµœì í™”
  
- **ë©”ëª¨ë¦¬ ê´€ë¦¬**
  - CUDA Graph API í™œìš©
  - Memory pool í™•ì¥
  - Unified Memory ìµœì í™”

#### **2. AI ëª¨ë¸ ê°œì„ **
- **ìƒˆë¡œìš´ YOLO ë²„ì „ ì§€ì›**
  - YOLOv13+ ì•„í‚¤í…ì²˜ í†µí•©
  - Transformer ê¸°ë°˜ ëª¨ë¸ ì§€ì›
  
- **TensorRT ìµœì í™”**
  - INT8 ì–‘ìí™” êµ¬í˜„
  - Dynamic shape ìµœì í™”
  - Plugin ë ˆì´ì–´ ê°œë°œ

#### **3. í•˜ë“œì›¨ì–´ ì§€ì› í™•ì¥**
- **ìƒˆë¡œìš´ ì…ë ¥ ì¥ì¹˜**
  ```cpp
  class NewDeviceInputMethod : public InputMethod {
      void move(int x, int y) override;
      void press() override;
      void release() override;
  };
  ```
  
- **Multi-GPU ì§€ì›**
  - NCCL í†µì‹  êµ¬í˜„
  - ë¡œë“œ ë°¸ëŸ°ì‹± ì•Œê³ ë¦¬ì¦˜

#### **4. ë¬¸ì„œí™” ë° ë„êµ¬**
- **ì„±ëŠ¥ ë¶„ì„ ë„êµ¬**
  - ì‹¤ì‹œê°„ í”„ë¡œíŒŒì¼ëŸ¬ UI
  - ìë™ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
  
- **ì‚¬ìš©ì ê°€ì´ë“œ**
  - ë¹„ë””ì˜¤ íŠœí† ë¦¬ì–¼
  - ì„¤ì • ìµœì í™” ê°€ì´ë“œ
  - ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ğŸ“‹ ì½”ë“œ ë¦¬ë·° ê¸°ì¤€

#### **í•„ìˆ˜ ê²€í†  ì‚¬í•­**
1. **ì„±ëŠ¥ ì˜í–¥ë„**
   - ë ˆì´í„´ì‹œ ì¦ê°€ ì—¬ë¶€
   - GPU ì‚¬ìš©ë¥  ë³€í™”
   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë³€í™”

2. **ì½”ë“œ í’ˆì§ˆ**
   - RAII íŒ¨í„´ ì¤€ìˆ˜
   - ì—ëŸ¬ ì²˜ë¦¬ ì ì ˆì„±
   - ì£¼ì„ ë° ë¬¸ì„œí™”

3. **í˜¸í™˜ì„±**
   - ë‹¤ì–‘í•œ GPU ì•„í‚¤í…ì²˜ ì§€ì›
   - Windows ë²„ì „ í˜¸í™˜ì„±
   - ì˜ì¡´ì„± ë²„ì „ ê´€ë¦¬

### ğŸ› ï¸ ìœ ìš©í•œ ë¦¬ì†ŒìŠ¤

- [CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/)
- [CUDA Best Practices](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)

### ğŸ† ê¸°ì—¬ì ì¸ì •

#### **ê¸°ì—¬ ë ˆë²¨**
- ğŸ¥‰ **Bronze**: ì²« PR ë¨¸ì§€
- ğŸ¥ˆ **Silver**: 5ê°œ ì´ìƒ PR ë¨¸ì§€
- ğŸ¥‡ **Gold**: ì£¼ìš” ê¸°ëŠ¥ êµ¬í˜„
- ğŸ’ **Diamond**: í•µì‹¬ ì„±ëŠ¥ ê°œì„ 

#### **ëª…ì˜ˆì˜ ì „ë‹¹**
ê¸°ì—¬ìë“¤ì€ READMEì˜ Contributors ì„¹ì…˜ì— ê¸°ë¡ë˜ë©°, ì£¼ìš” ê¸°ì—¬ìëŠ” í”„ë¡œì íŠ¸ ë©”ì¸í…Œì´ë„ˆë¡œ ì´ˆëŒ€ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ“ ì—°ë½ì²˜

- **GitHub**: [https://github.com/needitem/needaimbot](https://github.com/needitem/needaimbot)
- **Issues**: [GitHub Issues](https://github.com/needitem/needaimbot/issues)
- **Email**: th07290828@gmail.com

---

## ğŸ“Š ë¡œë“œë§µ

### Phase 1: Performance (í˜„ì¬)
- âœ… 3ms ì´ë‚´ ì¶”ë¡  ë‹¬ì„±
- âœ… Zero-Copy ìº¡ì²˜ êµ¬í˜„
- âœ… Triple Buffer ì‹œìŠ¤í…œ
- âœ… TensorRT 10.8 í†µí•©

### Phase 2: Features (ê³„íš)
- â¬œ Transformer ê¸°ë°˜ ëª¨ë¸ ì§€ì›
- â¬œ Multi-GPU ì§€ì›
- â¬œ Cloud ì¶”ë¡  ì˜µì…˜
- â¬œ ì‹¤ì‹œê°„ ëª¨ë¸ í•™ìŠµ

### Phase 3: Ecosystem (ë¯¸ë˜)
- â¬œ í”ŒëŸ¬ê·¸ì¸ ì‹œìŠ¤í…œ
- â¬œ ì›¹ ëŒ€ì‹œë³´ë“œ
- â¬œ ëª¨ë°”ì¼ ì»¨íŠ¸ë¡¤
- â¬œ API ì„œë¹„ìŠ¤

---

## ğŸ“„ ë¼ì´ì„ ìŠ¤

MIT License - ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ ì°¸ì¡°

---

## ğŸ™ ê°ì‚¬ì˜ ë§

ì´ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒ ê¸°ìˆ ê³¼ ì»¤ë®¤ë‹ˆí‹°ì˜ ë„ì›€ìœ¼ë¡œ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤:

- **NVIDIA**: CUDA, TensorRT, cuDNN
- **Microsoft**: DirectX, Windows SDK
- **Open Source**: Dear ImGui, Eigen, GLFW
- **Community**: ëª¨ë“  ê¸°ì—¬ìì™€ í…ŒìŠ¤í„°

---

<div align="center">

**Built with â¤ï¸ for Ultimate Performance**

*"ê·¹í•œì˜ íš¨ìœ¨ì„ ì¶”êµ¬í•˜ëŠ” ì°¨ì„¸ëŒ€ AI íƒ€ê²ŸíŒ… ì‹œìŠ¤í…œ"*

</div>